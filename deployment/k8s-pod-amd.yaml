apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-amd
  namespace: sow-template-service
  labels:
    app: ollama
    gpu-type: amd
    component: ai-inference
spec:
  replicas: 1  # Single instance for GPU workload
  strategy:
    type: Recreate  # Ensure only one instance runs at a time
  selector:
    matchLabels:
      app: ollama
      gpu-type: amd
  template:
    metadata:
      labels:
        app: ollama
        gpu-type: amd
        component: ai-inference
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "11434"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: sow-service-account
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        supplementalGroups: [44, 109]  # video and render groups
      nodeSelector:
        amd.com/gpu.present: "true"
        kubernetes.io/arch: amd64
      tolerations:
      - key: amd.com/gpu
        operator: Exists
        effect: NoSchedule
      - key: gpu-workload
        operator: Equal
        value: "true"
        effect: NoSchedule
      containers:
      - name: ollama
        image: ollama-amd:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 11434
          name: http
          protocol: TCP
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0:11434"
        - name: OLLAMA_MODELS
          value: "/models"
        - name: OLLAMA_KEEP_ALIVE
          value: "24h"
        - name: OLLAMA_MAX_LOADED_MODELS
          value: "3"
        - name: OLLAMA_NUM_PARALLEL
          value: "4"
        - name: OLLAMA_MAX_QUEUE
          value: "512"
        # AMD GPU specific environment variables
        - name: HIP_VISIBLE_DEVICES
          value: "all"
        - name: ROC_ENABLE_PRE_VEGA
          value: "1"
        - name: ROCM_PATH
          value: "/opt/rocm"
        - name: HCC_AMDGPU_TARGET
          value: "gfx1030,gfx1031,gfx1032,gfx1100,gfx1101,gfx1102"
        - name: HSA_OVERRIDE_GFX_VERSION
          value: "10.3.0"
        - name: GPU_DEVICE_ORDINAL
          value: "0"
        - name: ROCR_VISIBLE_DEVICES
          value: "0"
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1000
          runAsGroup: 1000
          readOnlyRootFilesystem: false  # ROCm needs write access
          capabilities:
            drop:
            - ALL
            add:
            - SYS_ADMIN  # Required for GPU access
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
            amd.com/gpu: 1
          limits:
            memory: "16Gi"
            cpu: "4000m"
            amd.com/gpu: 1
        volumeMounts:
        - name: models
          mountPath: /models
        - name: tmp
          mountPath: /tmp
        - name: rocm-dev
          mountPath: /dev/kfd
          readOnly: false
        - name: rocm-render
          mountPath: /dev/dri
          readOnly: false
        - name: rocm-libs
          mountPath: /opt/rocm
          readOnly: true
        - name: ollama-config
          mountPath: /etc/ollama
          readOnly: true
        readinessProbe:
          httpGet:
            path: /api/tags
            port: 11434
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        livenessProbe:
          httpGet:
            path: /api/version
            port: 11434
            scheme: HTTP
          initialDelaySeconds: 60
          periodSeconds: 20
          timeoutSeconds: 10
          successThreshold: 1
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /api/version
            port: 11434
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 30
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ollama-models-pvc
      - name: tmp
        emptyDir:
          sizeLimit: 1Gi
      - name: rocm-dev
        hostPath:
          path: /dev/kfd
          type: CharDevice
      - name: rocm-render
        hostPath:
          path: /dev/dri
          type: Directory
      - name: rocm-libs
        hostPath:
          path: /opt/rocm
          type: Directory
      - name: ollama-config
        configMap:
          name: ollama-amd-config
          defaultMode: 0644
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      schedulerName: default-scheduler
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-amd-service
  namespace: sow-template-service
  labels:
    app: ollama
    gpu-type: amd
spec:
  type: ClusterIP
  selector:
    app: ollama
    gpu-type: amd
  ports:
  - name: http
    port: 11434
    targetPort: 11434
    protocol: TCP
  sessionAffinity: None
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-amd-config
  namespace: sow-template-service
  labels:
    app: ollama
    gpu-type: amd
data:
  ollama.conf: |
    # Ollama configuration for AMD GPU
    OLLAMA_HOST=0.0.0.0:11434
    OLLAMA_MODELS=/models
    OLLAMA_KEEP_ALIVE=24h
    OLLAMA_MAX_LOADED_MODELS=3
    OLLAMA_NUM_PARALLEL=4
    OLLAMA_MAX_QUEUE=512
    
    # AMD GPU settings
    HIP_VISIBLE_DEVICES=all
    ROC_ENABLE_PRE_VEGA=1
    ROCM_PATH=/opt/rocm
    
    # Performance tuning
    OLLAMA_FLASH_ATTENTION=1
    OLLAMA_TENSOR_PARALLEL_SIZE=1
    OLLAMA_MAX_BATCH_SIZE=32
  
  gpu-info.sh: |
    #!/bin/bash
    echo "=== AMD GPU Information ==="
    rocm-smi --showallinfo
    echo "=== HIP Runtime Info ==="
    hipconfig --check
    echo "=== Available GPUs ==="
    rocm-smi --showid
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-amd-models-pvc
  namespace: sow-template-service
  labels:
    app: ollama
    gpu-type: amd
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd  # Adjust based on your storage class
  resources:
    requests:
      storage: 50Gi
  volumeMode: Filesystem
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ollama-amd-pdb
  namespace: sow-template-service
spec:
  minAvailable: 0  # Allow disruption since it's single replica
  selector:
    matchLabels:
      app: ollama
      gpu-type: amd
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ollama-amd-hpa
  namespace: sow-template-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ollama-amd
  minReplicas: 1
  maxReplicas: 1  # GPU workloads typically don't scale horizontally
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60